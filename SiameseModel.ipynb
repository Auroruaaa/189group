{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b372a17-603c-4b0e-b372-85e5e849c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eefee8-1a40-4c08-be63-f7bcf18fbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg_data(file_path):\n",
    "    raw = mne.io.read_raw_edf(file_path, preload = True, verbose = False)\n",
    "    raw.pick(['Oz..', 'T7..', 'Cz..'])    \n",
    "    raw.filter(1., 40., fir_design = 'firwin', verbose = False)\n",
    "\n",
    "    T_sec = 1\n",
    "    stride_sec = 4 / 160\n",
    "    offset_sec = 8/160\n",
    "    overlap = T_sec - stride_sec\n",
    "\n",
    "    epochs1 = mne.make_fixed_length_epochs(\n",
    "        raw,\n",
    "        duration=T_sec,\n",
    "        overlap=overlap,\n",
    "        preload=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    # epochs2 = mne.make_fixed_length_epochs(\n",
    "    #     raw,\n",
    "    #     duration=T_sec,\n",
    "    #     overlap=overlap,\n",
    "    #     preload=True,\n",
    "    #     verbose=False,\n",
    "    # )\n",
    "    # combined = mne.concatenate_epochs([epochs1, epochs2])\n",
    "    return epochs1\n",
    "\n",
    "eeg_file_path = 'data/files/eegmmidb/1.0.0/S003/S003R01.edf'\n",
    "epochs = load_eeg_data(eeg_file_path)\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25917848-8937-4f15-8c6f-a683ab02cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.plot(n_epochs = 10, n_channels = 3, scalings = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f3879-1255-4c2c-8808-4aca3cd32378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMotorImageryDataset(Dataset):\n",
    "    def __init__(self, epochs):\n",
    "        self.data = epochs.get_data()\n",
    "        self.labels = epochs.events[:,-1]\n",
    "\n",
    "        self.data = (self.data - np.mean(self.data, axis = 2, keepdims = True)) / np.std(self.data, axis = 2, keepdims = True)\n",
    "        self.data = self.data.astype(np.float32)\n",
    "\n",
    "        self.indices_by_class = {}\n",
    "        for i, label in enumerate(self.labels):\n",
    "            self.indices_by_class.setdefault(label, []).append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        sample = sample[np.newaxis,:,:]\n",
    "        return sample, label\n",
    "\n",
    "base_dataset = EEGMotorImageryDataset(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45e562-b422-4195-9e8f-1d90d655eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    sample, label = base_dataset[i]\n",
    "    # Remove the singleton dimension (1, n_channels, n_times) -> (n_channels, n_times)\n",
    "    sample = sample.squeeze(0)\n",
    "    n_channels, n_times = sample.shape\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for ch in range(n_channels):\n",
    "        plt.plot(sample[ch], label=f\"Channel {ch}\")\n",
    "    plt.title(f\"Sample {i} - Label: {label}\")\n",
    "    plt.xlabel(\"Time points\")\n",
    "    plt.ylabel(\"Normalized Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10762e4e-9bc9-41de-94d6-b567e31dffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseEEGMotorImageryDataset(Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.data = base_dataset.data\n",
    "        self.labels = base_dataset.labels\n",
    "        self.indices_by_class = base_dataset.indices_by_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample1, label1 = self.basedataset[idx]\n",
    "\n",
    "        similar = np.random.randit(0,2)\n",
    "\n",
    "        if similar:\n",
    "            indices = self.indices_by_class[label1]\n",
    "            idx2 = idx\n",
    "            while idx2 == idx:\n",
    "                idx2 = np.random.choice(indices)\n",
    "        else:\n",
    "            other_classes = list(self.indices_by_class.keys())\n",
    "            other_classes.remove(label1)\n",
    "            chosen_label = np.random.choice(other_classes)\n",
    "            idx2 = np.random.choice(self.indices_by_class[chosen_label])\n",
    "\n",
    "        sample2, label2 = self.base_dataset[idx2] \n",
    "        similarity = 1 if label1 == label2 else 0\n",
    "        return sample1, sample2, np.array([similarity], dtype = np.float32)\n",
    "\n",
    "siamese_dataset = SiameseEEGMotorImageryDataset(base_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac536ab-ff91-4c71-ab15-56b9f7431c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "loader = DataLoader(siamese_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb3847-4e28-4c63-b8c1-1250a00dd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 1\n",
    "F1 = 8\n",
    "D = 2\n",
    "F2 = 16\n",
    "\n",
    "kernel_size_1 = (1, 64)  \n",
    "kernel_padding_1 = (0, 32)\n",
    "kernel_size_2 = (2, 32)  \n",
    "kernel_avgpool_1 = (1, 8)\n",
    "dropout_rate = 0.5\n",
    "kernel_size_3 = (1, 16)  \n",
    "kernel_padding_3 = (0, 8)\n",
    "kernel_size_4 = (1, 1)   \n",
    "kernel_avgpool_2 = (1, 4)\n",
    "signal_length = 256      \n",
    "num_class = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a19e4-7f85-4236-8c38-5a492eac9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNetFeature(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(EEGNetFeature, self).__init__()\n",
    "        # Layer 1\n",
    "        self.conv2d = nn.Conv2d(num_input, F1, kernel_size=kernel_size_1, padding=kernel_padding_1)\n",
    "        self.Batch_normalization_1 = nn.BatchNorm2d(F1)\n",
    "        # Layer 2\n",
    "        self.Depthwise_conv2D = nn.Conv2d(F1, D * F1, kernel_size=kernel_size_2, groups=F1)\n",
    "        self.Batch_normalization_2 = nn.BatchNorm2d(D * F1)\n",
    "        self.Elu = nn.ELU()\n",
    "        self.Average_pooling2D_1 = nn.AvgPool2d(kernel_avgpool_1)\n",
    "        self.Dropout = nn.Dropout2d(dropout_rate)\n",
    "        # Layer 3\n",
    "        self.Separable_conv2D_depth = nn.Conv2d(D * F1, D * F1, kernel_size=kernel_size_3,\n",
    "                                                 padding=kernel_padding_3, groups=D * F1)\n",
    "        self.Separable_conv2D_point = nn.Conv2d(D * F1, F2, kernel_size=kernel_size_4)\n",
    "        self.Batch_normalization_3 = nn.BatchNorm2d(F2)\n",
    "        self.Average_pooling2D_2 = nn.AvgPool2d(kernel_avgpool_2)\n",
    "        # Layer 4\n",
    "        self.Flatten = nn.Flatten()\n",
    "        # Compute the flattened feature size. This depends on your input signal size.\n",
    "        # Here we assume the pooling operations reduce the time dimension by a factor of 32.\n",
    "        self.Dense = nn.Linear(F2 * (round(signal_length / 32)), num_class)\n",
    "        # Note: we remove the Softmax to get raw embeddings (or logits) for the siamese branch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        y = self.conv2d(x)\n",
    "        y = self.Batch_normalization_1(y)\n",
    "        # Layer 2\n",
    "        y = self.Depthwise_conv2D(y)\n",
    "        y = self.Batch_normalization_2(y)\n",
    "        y = self.Elu(y)\n",
    "        y = self.Average_pooling2D_1(y)\n",
    "        y = self.Dropout(y)\n",
    "        # Layer 3\n",
    "        y = self.Separable_conv2D_depth(y)\n",
    "        y = self.Separable_conv2D_point(y)\n",
    "        y = self.Batch_normalization_3(y)\n",
    "        y = self.Elu(y)\n",
    "        y = self.Average_pooling2D_2(y)\n",
    "        y = self.Dropout(y)\n",
    "        # Layer 4\n",
    "        y = self.Flatten(y)\n",
    "        y = self.Dense(y)\n",
    "        return y  # These are your embeddings (or logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba053b94-f6b0-4c6a-8a74-86a64d19d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseEEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseEEGNet, self).__init__()\n",
    "        # Shared EEGNet feature extractor (weights will be shared for both inputs)\n",
    "        self.feature_extractor = EEGNetFeature()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Get embeddings for both inputs\n",
    "        embed1 = self.feature_extractor(x1)\n",
    "        embed2 = self.feature_extractor(x2)\n",
    "        cos_sim = F.cosine_similarity(embed1, embed2, dim=1, eps=1e-6)\n",
    "        # Optionally, you can reshape it to (batch_size, 1) if needed.\n",
    "        return embed1, embed2, cos_sim.unsqueeze(1)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Create the Siamese model instance\n",
    "siamese_model = SiameseEEGNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03389839-d086-46db-8352-b406b73b2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have two inputs (e.g., two EEG signals) with shape [batch_size, channels, height, width]\n",
    "# For example, using random tensors (replace with your actual data)\n",
    "batch_size = 8\n",
    "# The input shape must match what EEGNetFeature expects. Here we assume a shape of (1, 1, 256) per sample.\n",
    "# If your EEG data is 2D (channels x signal_length), you might need to adjust the dimensions.\n",
    "input_shape = (num_input, 1, signal_length)  # example shape; adjust as needed\n",
    "\n",
    "x1 = torch.randn(batch_size, *input_shape)\n",
    "x2 = torch.randn(batch_size, *input_shape)\n",
    "\n",
    "# Forward pass through the Siamese network\n",
    "embed1, embed2, distance = siamese_model(x1, x2)\n",
    "print(\"Embedding 1 shape:\", embed1.shape)\n",
    "print(\"Embedding 2 shape:\", embed2.shape)\n",
    "print(\"Distance shape:\", distance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43ca6c-474c-4cbd-b7cc-bf23146d386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_contrastive_loss(y_true, cos_sim, margin = 0.275):\n",
    "    loss_similar = y_true * torch.pow((1-cos_sim), 2)\n",
    "\n",
    "    loss_dissimilar = (1-y_true) * torch.pow(torch.clamp(cos_sim - margin, min = 0.0), 2)\n",
    "    loss = torch.mean(loss_similar + loss_dissimlar)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe76e82-737c-42cd-953f-dce619a65e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate your model and move it to device.\n",
    "siamese_model = SiameseEEGNetCosine().to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 20\n",
    "\n",
    "# Split the dataset into training and validation (80%/20% split)\n",
    "dataset_size = len(siamese_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "train_dataset, val_dataset = random_split(siamese_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define optimizer (using Adam here)\n",
    "optimizer = optim.Adam(siamese_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Optionally, define a scheduler (e.g., ReduceLROnPlateau) if desired:\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# For tracking best validation loss to save the best model\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    siamese_model.train()  # set model to training mode\n",
    "    running_loss = 0.0\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training step\n",
    "    for batch_idx, (x1, x2, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        labels = labels.to(device)  # Expected shape: (batch_size, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute embeddings and cosine similarity\n",
    "        _, _, cos_sim = siamese_model(x1, x2)\n",
    "        \n",
    "        # Compute loss using cosine-based contrastive loss\n",
    "        loss = cosine_contrastive_loss(labels, cos_sim, margin=margin)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * x1.size(0)\n",
    "    \n",
    "    # Compute average training loss for the epoch\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation step (without gradient computations)\n",
    "    siamese_model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in val_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass on validation data\n",
    "            _, _, cos_sim = siamese_model(x1, x2)\n",
    "            loss = cosine_contrastive_loss(labels, cos_sim, margin=margin)\n",
    "            val_running_loss += loss.item() * x1.size(0)\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    \n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Time: {epoch_duration:.2f}s | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Optionally, step the scheduler:\n",
    "    # scheduler.step(val_loss)\n",
    "    \n",
    "    # Save the model if validation loss decreases\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(siamese_model.state_dict(), 'best_siamese_model.pth')\n",
    "        print(\"  --> Best model saved.\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
