{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b372a17-603c-4b0e-b372-85e5e849c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3129c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcec4bf",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62eefee8-1a40-4c08-be63-f7bcf18fbf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Annotations | 30 segments: T0 (15), T1 (7), T2 (8)>\n"
     ]
    }
   ],
   "source": [
    "def get_subject_id(file_path):\n",
    "    parts = file_path.split(os.sep)\n",
    "    subject = parts[-2]      # e.g., \"S003\"\n",
    "    subject_num = int(subject[1:])  # converts \"003\" to integer 3\n",
    "    return subject_num\n",
    "\n",
    "def load_eeg_data(file_path):\n",
    "    raw = mne.io.read_raw_edf(file_path, preload = True, verbose = False)\n",
    "    raw.pick(['Oz..', 'T7..', 'Cz..'])    \n",
    "    raw.filter(1., 40., fir_design = 'firwin', verbose = False)\n",
    "    # raw.crop(tmin=0, tmax=30) #remove cropping\n",
    "    return raw\n",
    "\n",
    "eeg_file_path = \"./files/S003/S003R03.edf\"\n",
    "raw = load_eeg_data(eeg_file_path)\n",
    "print(raw.annotations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa4ee5",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464f3879-1255-4c2c-8808-4aca3cd32378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMotorImageryDataset(Dataset):\n",
    "    def __init__(self, epochs, subject_id):\n",
    "        data = epochs.get_data()\n",
    "        valid_idx = [i for i, epoch in enumerate(data) if epoch.shape[-1] == 160]\n",
    "    \n",
    "        self.data = data[valid_idx]\n",
    "        self.labels = np.full((self.data.shape[0],), subject_id) \n",
    "\n",
    "        self.data = (self.data - np.mean(self.data, axis = 2, keepdims = True)) / np.std(self.data, axis = 2, keepdims = True)\n",
    "        self.data = self.data.astype(np.float32)\n",
    "        self.indices_by_class = {subject_id: list(range(len(self.data)))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        sample = sample[np.newaxis,:,:]\n",
    "        return sample, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c53634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_as_dataset(file_path):\n",
    "\n",
    "    raw = load_eeg_data(file_path)\n",
    "    sfreq = raw.info['sfreq']\n",
    "    expected_length = int(round(sfreq * 1.0))  # for 1 second epochs\n",
    "\n",
    "    epochs = mne.make_fixed_length_epochs(raw, duration=1.0, preload=True, verbose=False)\n",
    "\n",
    "\n",
    "    data = epochs.get_data() \n",
    "    valid_idx = [i for i, epoch in enumerate(data) if epoch.shape[-1] == expected_length]\n",
    "    filtered_epochs = epochs[valid_idx]\n",
    "\n",
    "    subject_id = get_subject_id(file_path)\n",
    "    return EEGMotorImageryDataset(filtered_epochs, subject_id=subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f621a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(folder_path):\n",
    "    edf_files = []\n",
    "    for root, dir, files in os.walk(folder_path):\n",
    "        for f in files:\n",
    "            if f.lower().endswith('.edf'):\n",
    "                edf_files.append(os.path.join(root, f))\n",
    "\n",
    "    datasets = []\n",
    "    for file_path in tqdm(edf_files):\n",
    "        ds = load_file_as_dataset(file_path)\n",
    "        datasets.append(ds)\n",
    "    if datasets:\n",
    "        combined_dataset = ConcatDataset(datasets)\n",
    "        return combined_dataset\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7ce8a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1526/1526 [01:14<00:00, 20.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in combined_dataset: 170272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./files\"\n",
    "dataset = load_all_data(folder_path)\n",
    "\n",
    "if dataset is None:\n",
    "    print('No EDF files found')\n",
    "else: \n",
    "    print(\"Total samples in combined_dataset:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465e296",
   "metadata": {},
   "source": [
    "## Siamse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10762e4e-9bc9-41de-94d6-b567e31dffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseEEGMotorImageryDataset(Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.indices_by_subject = {}\n",
    "        for idx in range(len(self.base_dataset)):\n",
    "            _, label = self.base_dataset[idx]\n",
    "            if label not in self.indices_by_subject:\n",
    "                self.indices_by_subject[label] = []\n",
    "            self.indices_by_subject[label].append(idx)\n",
    "        self.all_subjects = list(self.indices_by_subject.keys())\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample1, label1 = self.base_dataset[idx]\n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            indices = self.indices_by_subject[label1]\n",
    "            canidate = np.random.choice([i for i in indices if i != idx])\n",
    "            sample2, _ = self.base_dataset[canidate]\n",
    "            pair_label = 1\n",
    "        else:\n",
    "            other_subjects = [sub for sub in self.all_subjects if sub != label1]\n",
    "            negative_label = np.random.choice(other_subjects)\n",
    "            indices = self.indices_by_subject[negative_label]\n",
    "            canidate = np.random.choice(indices)\n",
    "            sample2, _ = self.base_dataset[canidate]\n",
    "            pair_label = 0\n",
    "\n",
    "        return (sample1, sample2), pair_label\n",
    "\n",
    "siamese_dataset = SiameseEEGMotorImageryDataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64b9cd",
   "metadata": {},
   "source": [
    "## EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eceb3847-4e28-4c63-b8c1-1250a00dd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 1\n",
    "F1 = 8\n",
    "D = 2\n",
    "F2 = 16\n",
    "\n",
    "kernel_size_1 = (1, 64)  \n",
    "kernel_padding_1 = (0, 32)\n",
    "kernel_size_2 = (2, 32)  \n",
    "kernel_avgpool_1 = (1, 8)\n",
    "dropout_rate = 0.5\n",
    "kernel_size_3 = (1, 16)  \n",
    "kernel_padding_3 = (0, 8)\n",
    "kernel_size_4 = (1, 1)   \n",
    "kernel_avgpool_2 = (1, 4)\n",
    "signal_length = 160      \n",
    "embedding_dim = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040a19e4-7f85-4236-8c38-5a492eac9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNetFeature(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(EEGNetFeature, self).__init__()\n",
    "        # Layer 1\n",
    "        self.conv2d = nn.Conv2d(num_input, F1, kernel_size=kernel_size_1, padding=kernel_padding_1)\n",
    "        self.Batch_normalization_1 = nn.BatchNorm2d(F1)\n",
    "        # Layer 2\n",
    "        self.Depthwise_conv2D = nn.Conv2d(F1, D * F1, kernel_size=kernel_size_2, groups=F1)\n",
    "        self.Batch_normalization_2 = nn.BatchNorm2d(D * F1)\n",
    "        self.Elu = nn.ELU()\n",
    "        self.Average_pooling2D_1 = nn.AvgPool2d(kernel_avgpool_1)\n",
    "        self.Dropout = nn.Dropout2d(dropout_rate)\n",
    "        # Layer 3\n",
    "        self.Separable_conv2D_depth = nn.Conv2d(D * F1, D * F1, kernel_size=kernel_size_3,\n",
    "                                                 padding=kernel_padding_3, groups=D * F1)\n",
    "        self.Separable_conv2D_point = nn.Conv2d(D * F1, F2, kernel_size=kernel_size_4)\n",
    "        self.Batch_normalization_3 = nn.BatchNorm2d(F2)\n",
    "        self.Average_pooling2D_2 = nn.AvgPool2d(kernel_avgpool_2)\n",
    "        # Layer 4\n",
    "        self.Flatten = nn.Flatten()\n",
    "        # Compute the flattened feature size. This depends on your input signal size.\n",
    "        # Here we assume the pooling operations reduce the time dimension by a factor of 32.\n",
    "        self.Dense = nn.Linear(128, embedding_dim)\n",
    "        # Note: we remove the Softmax to get raw embeddings (or logits) for the siamese branch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        y = self.conv2d(x)\n",
    "        y = self.Batch_normalization_1(y)\n",
    "        # Layer 2\n",
    "        y = self.Depthwise_conv2D(y)\n",
    "        y = self.Batch_normalization_2(y)\n",
    "        y = self.Elu(y)\n",
    "        y = self.Average_pooling2D_1(y)\n",
    "        y = self.Dropout(y)\n",
    "        # Layer 3\n",
    "        y = self.Separable_conv2D_depth(y)\n",
    "        y = self.Separable_conv2D_point(y)\n",
    "        y = self.Batch_normalization_3(y)\n",
    "        y = self.Elu(y)\n",
    "        y = self.Average_pooling2D_2(y)\n",
    "        y = self.Dropout(y)\n",
    "        # Layer 4\n",
    "        y = self.Flatten(y)\n",
    "        y = self.Dense(y)\n",
    "        return y  # These are your embeddings (or logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494998fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_input, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(1,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(61440, 1024)\n",
    "        self.dense2 = nn.Linear(1024, 128)\n",
    "        self.dropout = nn.Dropout2d(0.5) #hyperparameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ad629",
   "metadata": {},
   "source": [
    "## SiameseEEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba053b94-f6b0-4c6a-8a74-86a64d19d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseEEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseEEGNet, self).__init__()\n",
    "        # Shared EEGNet feature extractor (weights will be shared for both inputs)\n",
    "        self.feature_extractor = EEGNetFeature()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Get embeddings for both inputs\n",
    "        embed1 = self.feature_extractor(x1)\n",
    "        embed2 = self.feature_extractor(x2)\n",
    "        return embed1, embed2\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Create the Siamese model instance\n",
    "siamese_model = SiameseEEGNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ebe3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseCNN, self).__init__()\n",
    "        # Shared EEGNet feature extractor (weights will be shared for both inputs)\n",
    "        self.feature_extractor = CNN()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Get embeddings for both inputs\n",
    "        embed1 = self.feature_extractor(x1)\n",
    "        embed2 = self.feature_extractor(x2)\n",
    "        return embed1, embed2\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Create the Siamese model instance\n",
    "siamese_model2 = SiameseCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b30e55",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe43ca6c-474c-4cbd-b7cc-bf23146d386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, cos_sim, margin = 1.0):\n",
    "    loss_similar = y_true * torch.pow((1-cos_sim), 2)\n",
    "\n",
    "    loss_dissimilar = (1-y_true) * torch.pow(torch.clamp(cos_sim - margin, min = 0.0), 2)\n",
    "    loss = torch.mean(loss_similar + loss_dissimilar)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d763ce1",
   "metadata": {},
   "source": [
    "### Training Loss and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fe76e82-737c-42cd-953f-dce619a65e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m contrastive_loss(labels, cos_sim, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization step\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     56\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aabla\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aabla\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aabla\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate your model and move it to device.\n",
    "siamese_model = SiameseCNN().to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "threshold = 0.725\n",
    "# Split the dataset into training and validation (80%/20% split)\n",
    "dataset_size = len(siamese_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(siamese_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define optimizer (using Adam here)\n",
    "optimizer = torch.optim.Adam(siamese_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Optionally, define a scheduler (e.g., ReduceLROnPlateau) if desired:\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# For tracking best validation loss to save the best model\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    siamese_model.train()  # set model to training mode\n",
    "    running_loss = 0.0\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training step\n",
    "    for batch_idx, ((x1, x2), labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        labels = labels.to(device)  # Expected shape: (batch_size, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute embeddings and cosine similarity\n",
    "        embed1, embed2 = siamese_model(x1, x2)\n",
    "        \n",
    "        # Compute loss using cosine-based contrastive loss\n",
    "        cos_sim = F.cosine_similarity(embed1, embed2, dim=1, eps=1e-6).unsqueeze(1)\n",
    "        loss = contrastive_loss(labels, cos_sim, margin=0.9)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * x1.size(0)\n",
    "    \n",
    "    # Compute average training loss for the epoch\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation step (without gradient computations)\n",
    "    siamese_model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (x1, x2), labels in val_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            # Forward pass on validation data\n",
    "            embed1, embed2 = siamese_model(x1, x2)\n",
    "            cos_sim = F.cosine_similarity(embed1, embed2, dim=1, eps=1e-6).unsqueeze(1)\n",
    "            loss = contrastive_loss(labels, cos_sim, margin=0.9)\n",
    "            val_running_loss += loss.item() * x1.size(0)\n",
    "\n",
    "            preds = (cos_sim > threshold).float()\n",
    "            correct += (preds == labels).float().sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Time: {epoch_duration:.2f}s | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Optionally, step the scheduler:\n",
    "    # scheduler.step(val_loss)\n",
    "    \n",
    "    # Save the model if validation loss decreases\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(siamese_model.state_dict(), 'best_siamese_model.pth')\n",
    "        print(\"  --> Best model saved.\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a971e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
